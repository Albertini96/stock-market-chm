{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_retriever import DataRetriever\n",
    "from pre_processing import PreProcessing\n",
    "from scalers.min_max import MinMax\n",
    "from regressors.lstm_regressor import LSTMRegressor\n",
    "from regressors.esn_regressor import ESNRegressor\n",
    "import pandas as pd\n",
    "from model_evaluator import ModelEvaluator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Sequential\n",
    "from config import Config\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from decomposers.wavelet_transform import WaveletDecomposition\n",
    "from chm.chm import CascadeHierarquicalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing                 import List\n",
    "from chm.level              import CHMLevel\n",
    "from chm.stages             import CHMStage\n",
    "from decomposers.decomposer import BaseDecomposer\n",
    "from regressors.regressor   import BaseRegressor\n",
    "from copy import copy\n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "class CascadeHierarquicalModel():\n",
    "    \"\"\"\n",
    "    Cascade Hierarquical Model used to extract context from timeseries and predict non stationary timeseries\n",
    "\n",
    "    parameters : BaseRegressor    (Instanciated base regressor used to extract context)\n",
    "                 BaseRegressor    (Instanciated base regressor used to join context created from the stages)\n",
    "                 SeriesDecomposer (decomposition used to extract frequency from time series)\n",
    "                 int              (number of levels in the hierarquical model)\n",
    "                 int              (number of stages in the hierarquical model)\n",
    "                 bool             (use frequency extracted from time series as a feature)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,  ds:DataFrame                      ,\n",
    "                        x_cols:List[str]                  ,\n",
    "                        y_cols:str                        ,\n",
    "                        context_regressor:BaseRegressor   , \n",
    "                        stage_regressor:BaseRegressor     ,\n",
    "                        decomposer:BaseDecomposer         ,\n",
    "                        dec_cols:List[str]                ,\n",
    "                        num_stages:int=1                  ,\n",
    "                        use_frequency:bool=False\n",
    "                ):\n",
    "        self._ds                = ds\n",
    "        self._x_cols            = x_cols\n",
    "        self._y_cols            = y_cols\n",
    "        self._context_regressor = context_regressor\n",
    "        self._stage_regressor   = stage_regressor\n",
    "        self._decomposer        = decomposer\n",
    "        self._dec_cols          = dec_cols\n",
    "        self._num_stages        = num_stages\n",
    "        self._use_frequency     = use_frequency\n",
    "        self._list_stages       = list[CHMStage]\n",
    "\n",
    "        \n",
    "    def extract_context(self) -> None:\n",
    "        # Decomposing series\n",
    "        dec:BaseDecomposer = self._decomposer(\n",
    "                                ds         = self._ds,\n",
    "                                apply_cols = self._dec_cols\n",
    "                                )\n",
    "\n",
    "        # Obtaining the biggest decomposition\n",
    "        max_wave = 0\n",
    "        for i in dec.dict_waves.keys():    \n",
    "            if(len(dec.dict_waves[i]) > max_wave):\n",
    "                max_wave = len(dec.dict_waves[i])\n",
    "\n",
    "        # For each stage\n",
    "        for stage in range(self._num_stages):\n",
    "\n",
    "            # List of levels for this stage\n",
    "            temp_level_list = list[CHMLevel]\n",
    "\n",
    "            # For each level in the current stage\n",
    "            for level in range(max_wave):\n",
    "                \n",
    "                temp_ds = self._ds.copy()\n",
    "                temp_ds = temp_ds.drop(self._dec_cols, axis=1)\n",
    "\n",
    "                for wave in dec.dict_waves.keys():\n",
    "                    \n",
    "\n",
    "                temp_level = CHMLevel(self._context_regressor(\n",
    "                                                                ds=train, \n",
    "                                                                xcols=x_cols, \n",
    "                                                                y_cols=y_cols, \n",
    "                                                                n_inputs=7, \n",
    "                                                                n_features=len(x_cols), \n",
    "                                                                epochs=50\n",
    "                                                             )\n",
    "                                        )\n",
    "                temp_level_list.append(temp_level)\n",
    "\n",
    "            self._list_stages.append(CHMStage(temp_level_list, copy(self._stage_regressor)))\n",
    "\n",
    "    def __sintetize_series(self):\n",
    "        pass\n",
    "\n",
    "    def __recompose_series(self):\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  7 of 7 completed\n"
     ]
    }
   ],
   "source": [
    "#Retrieving data from yahoo API\n",
    "a = DataRetriever()\n",
    "a.get_yahoo_stock_data()\n",
    "\n",
    "#Defining which stocks will be predicted\n",
    "#x_cols_ = [x for x in b.columns.difference(['Date']) if x[-5:] == 'Close']\n",
    "predict_cols = ['ITUB3.SA_Close']\n",
    "\n",
    "\n",
    "#Obtaining yahoo dataset\n",
    "dataset = a.get_stock_ds()\n",
    "\n",
    "#Setting up column to be predicted\n",
    "y_cols = predict_cols\n",
    "#Setting up column to be used as features\n",
    "x_cols = dataset.columns.difference(['Date'] + list(y_cols))\n",
    "\n",
    "#Preprocessing yahoo data\n",
    "pp = PreProcessing(dataset, MinMax)\n",
    "train, test = pp.pre_process()\n",
    "\n",
    "#Training model to preprocessed data\n",
    "#reg = LSTMRegressor(ds=train, xcols=x_cols, y_cols=y_cols, n_inputs=7, n_features=len(x_cols), epochs=50)\n",
    "# reg = ESNRegressor(ds=train, xcols=x_cols, y_cols=y_cols, n_inputs=7, n_features=len(x_cols), epochs=50)\n",
    "# reg.fit_generator()\n",
    "\n",
    "# #Predicting data with trained model\n",
    "# reg_pred = reg.predict_generator(test)\n",
    "\n",
    "# #Adding predicted data to original dataset\n",
    "# reg_pred = pp._scalers[y_cols].inverse_transform(reg_pred)\n",
    "# pred = pd.DataFrame(reg_pred)\n",
    "# pred.columns = ['Prediction']\n",
    "# begin_idx  = len(dataset) - len(pred)\n",
    "# finish_idx = len(dataset) - 1\n",
    "# pred.index = list(range(begin_idx, finish_idx + 1))\n",
    "# dataset = dataset.join(pred)\n",
    "\n",
    "# #Evaluating model\n",
    "# me = ModelEvaluator(model_name='ESN', \n",
    "#                     data_set=dataset, \n",
    "#                     pred_col='Prediction', \n",
    "#                     y_col=y_cols, \n",
    "#                     x_col='Date'\n",
    "#                     )\n",
    "# #me.plot_results(False, True)\n",
    "# me.plot_results_predicted(False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = WaveletDecomposition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Mestrado\\stock-market-chm\\src\\decomposers\\wavelet_transform.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  ds[new_col] = rec[:len(rec)-1]\n"
     ]
    }
   ],
   "source": [
    "decompose_cols = list(train.filter(regex=(\"Open\\\\b|High\\\\b|Low\\\\b|Close\\\\b\")).columns)\n",
    "wd.decompose_series(ds=train,\n",
    "                    apply_cols=decompose_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSFT_Close_wave_0',\n",
       " 'MSFT_Close_wave_1',\n",
       " 'MSFT_Close_wave_2',\n",
       " 'MSFT_Close_wave_3',\n",
       " 'MSFT_Close_wave_4',\n",
       " 'MSFT_Close_wave_5',\n",
       " 'MSFT_Close_wave_6',\n",
       " 'MSFT_Close_wave_7',\n",
       " 'MSFT_Close_wave_8']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd.dict_waves['MSFT_Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the biggest decomposition\n",
    "max_wave = 0\n",
    "for i in wd.dict_waves.keys():\n",
    "    if(len(wd.dict_waves[i]) > max_wave):\n",
    "        max_wave = len(wd.dict_waves[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = train.columns.difference(['Date'])\n",
    "y_cols = ['ITUB3.SA_Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['7011.T_Close', '7011.T_Close_BBL_5_2.0', '7011.T_Close_BBM_5_2.0',\n",
       "       '7011.T_Close_BBU_5_2.0', '7011.T_Close_RSI', '7011.T_Close_SMA_5',\n",
       "       '7011.T_Close_wave_0', '7011.T_Close_wave_1', '7011.T_Close_wave_2',\n",
       "       '7011.T_Close_wave_3',\n",
       "       ...\n",
       "       'MSFT_Open_wave_1', 'MSFT_Open_wave_2', 'MSFT_Open_wave_3',\n",
       "       'MSFT_Open_wave_4', 'MSFT_Open_wave_5', 'MSFT_Open_wave_6',\n",
       "       'MSFT_Open_wave_7', 'MSFT_Open_wave_8', 'MSFT_STOCHd_14_3_3',\n",
       "       'MSFT_STOCHk_14_3_3'],\n",
       "      dtype='object', length=329)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1692/2425071721.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m chm = CascadeHierarquicalModel(context_regressor = ESNRegressor,\n\u001b[0m\u001b[0;32m      2\u001b[0m                                 \u001b[0mstage_regressor\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mESNRegressor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                 \u001b[0mdecomposer\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0mWaveletDecomposition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                 \u001b[0mnum_levels\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0mmax_wave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1692/1999787051.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, context_regressor, stage_regressor, decomposer, num_levels, num_stages, use_frequency)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mtemp_level_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCHMLevel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_levels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[0mtemp_level_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHMLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_regressor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_list_stages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHMStage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_level_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage_regressor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "chm = CascadeHierarquicalModel( X                 = train[x_cols],\n",
    "                                y                 = train[y_cols],\n",
    "                                context_regressor = ESNRegressor,\n",
    "                                stage_regressor   = ESNRegressor,\n",
    "                                decomposer        = WaveletDecomposition,\n",
    "                                num_levels        = max_wave \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSFT_Open': 9,\n",
       " 'MSFT_High': 9,\n",
       " 'MSFT_Low': 9,\n",
       " 'MSFT_Close': 9,\n",
       " 'AMZN_Open': 9,\n",
       " 'AMZN_High': 9,\n",
       " 'AMZN_Low': 9,\n",
       " 'AMZN_Close': 9,\n",
       " 'B3SA3.SA_Open': 9,\n",
       " 'B3SA3.SA_High': 9,\n",
       " 'B3SA3.SA_Low': 9,\n",
       " 'B3SA3.SA_Close': 9,\n",
       " 'ABEV3.SA_Open': 9,\n",
       " 'ABEV3.SA_High': 9,\n",
       " 'ABEV3.SA_Low': 9,\n",
       " 'ABEV3.SA_Close': 9,\n",
       " 'ITUB3.SA_Open': 9,\n",
       " 'ITUB3.SA_High': 9,\n",
       " 'ITUB3.SA_Low': 9,\n",
       " 'ITUB3.SA_Close': 9,\n",
       " 'AAPL_Open': 9,\n",
       " 'AAPL_High': 9,\n",
       " 'AAPL_Low': 9,\n",
       " 'AAPL_Close': 9,\n",
       " '7011.T_Open': 9,\n",
       " '7011.T_High': 9,\n",
       " '7011.T_Low': 9,\n",
       " '7011.T_Close': 9}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd.count_waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSFT_Open', 'MSFT_High', 'MSFT_Low', 'MSFT_Close', 'AMZN_Open',\n",
       "       'AMZN_High', 'AMZN_Low', 'AMZN_Close', 'B3SA3.SA_Open', 'B3SA3.SA_High',\n",
       "       'B3SA3.SA_Low', 'B3SA3.SA_Close', 'ABEV3.SA_Open', 'ABEV3.SA_High',\n",
       "       'ABEV3.SA_Low', 'ABEV3.SA_Close', 'ITUB3.SA_Open', 'ITUB3.SA_High',\n",
       "       'ITUB3.SA_Low', 'ITUB3.SA_Close', 'AAPL_Open', 'AAPL_High', 'AAPL_Low',\n",
       "       'AAPL_Close', '7011.T_Open', '7011.T_High', '7011.T_Low',\n",
       "       '7011.T_Close', '7011.T_Close_BBL_5_2.0', '7011.T_Close_BBM_5_2.0',\n",
       "       '7011.T_Close_BBU_5_2.0', 'AAPL_Close_BBL_5_2.0',\n",
       "       'AAPL_Close_BBM_5_2.0', 'AAPL_Close_BBU_5_2.0',\n",
       "       'ABEV3.SA_Close_BBL_5_2.0', 'ABEV3.SA_Close_BBM_5_2.0',\n",
       "       'ABEV3.SA_Close_BBU_5_2.0', 'AMZN_Close_BBL_5_2.0',\n",
       "       'AMZN_Close_BBM_5_2.0', 'AMZN_Close_BBU_5_2.0',\n",
       "       'B3SA3.SA_Close_BBL_5_2.0', 'B3SA3.SA_Close_BBM_5_2.0',\n",
       "       'B3SA3.SA_Close_BBU_5_2.0', 'ITUB3.SA_Close_BBL_5_2.0',\n",
       "       'ITUB3.SA_Close_BBM_5_2.0', 'ITUB3.SA_Close_BBU_5_2.0',\n",
       "       'MSFT_Close_BBL_5_2.0', 'MSFT_Close_BBM_5_2.0', 'MSFT_Close_BBU_5_2.0',\n",
       "       '7011.T_Close_SMA_5', 'AAPL_Close_SMA_5', 'ABEV3.SA_Close_SMA_5',\n",
       "       'AMZN_Close_SMA_5', 'B3SA3.SA_Close_SMA_5', 'ITUB3.SA_Close_SMA_5',\n",
       "       'MSFT_Close_SMA_5', '7011.T_Close_RSI', 'AAPL_Close_RSI',\n",
       "       'ABEV3.SA_Close_RSI', 'AMZN_Close_RSI', 'B3SA3.SA_Close_RSI',\n",
       "       'ITUB3.SA_Close_RSI', 'MSFT_Close_RSI'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.filter(regex=(\"Open|High|Low|Close\")).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ae4ca79f28d94746ca1e5c20c0c40afd365a28020698a392ef136b783d81a51"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('imp-virtual-env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
